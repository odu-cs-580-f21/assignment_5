{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS580-assignment_5",
      "provenance": [],
      "collapsed_sections": [
        "3kvrOyLIpb0q",
        "u1fg4VIavj1b",
        "frPCJq1H2DnP"
      ],
      "authorship_tag": "ABX9TyOqOF88jzEuvsVfsZZFD8eG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odu-cs-580-f21/assignment_5/blob/master/README.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 480/580 - Assignment_5 - DÃ¡niel B. Papp"
      ],
      "metadata": {
        "id": "F9kcU2EOtORd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notice\n",
        "\n",
        "Since I wasn't able to use the dataset provided in the assignment document, I used the same dataset but provided by Kaggle in a `.csv` format. The data and the Kaggle challenge can be found [here](https://www.kaggle.com/c/digit-recognizer/data)."
      ],
      "metadata": {
        "id": "CcUpuTswtMj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run\n",
        "\n",
        "```console\n",
        "python3 app.py\n",
        "```"
      ],
      "metadata": {
        "id": "k2ZXh1UQtKhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How it works\n",
        "\n",
        "On a high level, the program reads the `.csv` dataset and makes predictions for each image. The predictions are then compared to the actual labels and the accuracy is calculated. The accuracy is then printed to the terminal.\n",
        "\n",
        "All the necessary mathematical operations will be explained function by function below."
      ],
      "metadata": {
        "id": "4PdXawsBtEfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = pd.read_csv('./train.csv')\n",
        "data = np.array(data)"
      ],
      "metadata": {
        "id": "NrdDm_L1ekWD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network (NN) structure overview\n",
        "\n",
        "Our NN will have three layers in total. To further improve the accuracy of our model, we could increase the number or layers or the number of nodes in the hidden layer.\n",
        "\n",
        "1. Input layer\n",
        "   - The input layer has 784 nodes, which corresponds with the 28x28 pixels of the image. Each individual pixel is normalized to a value between 0 and 1. Since we only care about the pixel either being `rgb(0,0,0)` or `rgb(255,255,255)`, we can normalize each pixel to represent true or false (1 or 0) weather it is black or white.\n",
        "2. Hidden layer\n",
        "   - For the sake of simplicity, we will use a single hidden layer with 10 nodes. The value of each node is calculated based on weights and biases applied to the value of the 784 nodes in the input layer.\n",
        "3. Output layer\n",
        "   - The output layer has 10 nodes, each representing a digit from 0 through 9. The value of each node is calculated based on weights and biases applied to the value of the 10 nodes in the hidden layer using a softmax activation algorithm."
      ],
      "metadata": {
        "id": "AGhHTCMltBPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = data.shape\n",
        "np.random.shuffle(data) \n",
        "\n",
        "data_dev = data[0:1000].T\n",
        "Y_dev = data_dev[0]\n",
        "X_dev = data_dev[1:n]\n",
        "X_dev = X_dev / 255.\n",
        "\n",
        "data_train = data[1000:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255.\n",
        "_,m_train = X_train.shape"
      ],
      "metadata": {
        "id": "q85j05UFes8o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical operations and functions"
      ],
      "metadata": {
        "id": "HtYH4OYks6V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our data\n",
        "\n",
        "Each of our training example can be represented as a vector of 784 values. These vectors are then stacked into a matrix so we can calculate error from all examples at once with matrix operations.\n",
        "\n",
        "Our matrix will have an $m {\\times} n$ dimension, where m is the number of training examples and n is the number of nodes in the input layer (784). We transpose the matrix so the dimensions will be $n {\\times} m$, with each column corresponding to a training example and each row is a node."
      ],
      "metadata": {
        "id": "yjBbuV2Ssz8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight and biases\n",
        "\n",
        "Between every two layers is a set of connections between every node in the previous layer and every node in the following layer. Which means there is a weight of of $n^{[l]}$ for every `i` in the number of nodes in the previous layer and every `j` in the number of nodes in the following layer.\n",
        "\n",
        "From this we can conclude that our weights as a matrix will be $n^{[l]}{\\times}n^{[l - 1]}$ where $n^{[l]}$ is the number of nodes in the previous layer and $n^{[l - 1]}$ is the number of nodes in the following layer. We call this matrix the $W^{[l]}$ matrix corresponding to layer ${l}$ of our network.\n",
        "\n",
        "- $W^{[1]}$ will be represneting a $10 {\\times} 784$ matrix, taking 784 nodes from the input layer corresponding to 10 nodes in the hidden layer.\n",
        "- $W^{[2]}$ will have the dimensions of $10{\\times}10$"
      ],
      "metadata": {
        "id": "3kvrOyLIpb0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initParams():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "qZMB5sLThtHR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward propagation\n",
        "\n",
        "The first part of the function that is needed to calculate the forward propogation is the unactived values of the nodes in the first hidden layer. We can calculate that by applying $W^{[1]}$ and $b^{[1]}$ to our input layer. \n",
        "\n",
        "This produces $Z^{[1]} = W^{[1]} X + b^{[1]}$ where $X$ has the dimentions of $784{\\times}m$, and $W^{[1]}$ has the dimentions of $10{\\times}784$.\n",
        "\n",
        "$W^{[1]} X$ is the dot product between the two, yielding a new matrix of dimensions $10{\\times}m$. \n",
        "\n",
        "Our bias term $b^{[1]}$ has dimensions of $10{\\times}1$, but our goal is to apply the same column of bias to all $m$ columns of traning data. This means that $b^{[1]}$ is effectivly $10{\\times}m$ when calculating $Z^{[1]}$, which matches the dimensions of $W^{[1]} X$. "
      ],
      "metadata": {
        "id": "u1fg4VIavj1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ReLU(x)` (Rectified Linear Unit) activation function\n",
        "\n",
        "This function is a simple non-linear activation function that we apply to $Z^{[1]}$, before it goes to our next layer. \n",
        "\n",
        "This function is linear when the input value is above 0, and outputs 0 otherwise. This is enough to prevent our model from turning into a linear regression model. \n",
        "\n",
        "$$A^{[1]} = {\\text{ReLU}}(Z^{[1]}))$$\n",
        "\n",
        "The output of this function will be called $A^{[1]}$, which holds the values of the nodes in the hidden layer of our NN after applying our activation function to it. \n"
      ],
      "metadata": {
        "id": "kX2rQyp7fw_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)"
      ],
      "metadata": {
        "id": "KRdLnSMKh5El"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calculating $A^{[1]}$ we can proceed to calculate the values for our output layer. We call the result of that function $Z^{[2]}$. \n",
        "\n",
        "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
        "\n",
        "Since $Z^{[2]}$ is our output layer, we need to apply our `softmax(x)` activation function to it. "
      ],
      "metadata": {
        "id": "vFrc6jU2lX7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `softmax(x) activation function`\n",
        "\n",
        "This function takes a column of data at a time, taking each element in the column and outputting the exponential of that element divided by the sum of the exponentials of each of the elements in the input column. The result is a float value representing our probability (between 0, 1).\n",
        "\n",
        "$$\\frac {e^{z_i}}{{\\sum^K_{j=1}e^{z_j}}} $$\n",
        "\n",
        "The result of this function will be called $A^{[2]}$.\n",
        "\n",
        "$$A^{[2]} = {\\text{softmax}}(Z^{[2]})$$\n",
        "\n"
      ],
      "metadata": {
        "id": "z92zEDQKloqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A"
      ],
      "metadata": {
        "cellView": "code",
        "id": "YXJqjnWNiCLJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with this completed, our forward propagation is completed. "
      ],
      "metadata": {
        "id": "8FTQeGM0sYfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardProp(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "wOHZnZHFiEpF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ld23vSvpsqDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward Propagation\n",
        "\n",
        "The prupose of this function is to nudge our parameters to carry out gradient descent. Mathematically we are calulating the derivative of the loss function with respect to each weight and bias parameter. \n",
        "\n",
        "$$\n",
        "J(\\hat{y}, y) = -{\\sum^c_{i=0}}y_i log(\\hat{y}_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "gsL7uSx9sfD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy loss function\n",
        "\n",
        "This is a helper function that classifies the products that result from the softmax activation function. In our function $\\hat{y}$ represents our prediction vector, which looks like this:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "  0.01 \\\\\n",
        "  0.02 \\\\\n",
        "  0.05 \\\\\n",
        "  0.02 \\\\\n",
        "  0.80 \\\\\n",
        "  0.01 \\\\\n",
        "  0.01 \\\\\n",
        "  0.00 \\\\\n",
        "  0.01 \\\\\n",
        "  0.07 \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "kM5eVIVltAOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivative `ReLU(x)` activation function\n",
        "\n",
        "$$g^{[1]'}(Z^{[1]})$$\n",
        "\n",
        "Wehn the input value is greater than 0, the function is linear with a derivative of 1. When the input value is less than 0, the function is horizontal with a derivative of 0. \n",
        "\n",
        "Thus the result of this function is a matrix of 1s and 0s based on the values of $Z^{[1]}$."
      ],
      "metadata": {
        "id": "H2YOnZCZ1WTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLUDeriv(Z):\n",
        "    return Z > 0"
      ],
      "metadata": {
        "id": "Dg1OK7Oi1UWS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot encoding function\n",
        "\n",
        "In our backward propagation function, this is reprensted as $y$. This function correctly labels our training data while remaining a vector. The result in a example where our training node is 4, the one-hot encoding would return a vector like this:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  1 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "  0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Notice that based on our backward propagation function $y_i = 0$ for all $i$ except the correct label, in our example above it is 4. Which means that the loss for the given example is just the log of the probability given for the correct prediction. \n",
        "\n",
        "In our example above $J(\\hat{y}, y) = -\\log({y}_4) = -\\log(0.80) \\approx 0.097$.\n",
        "\n",
        "We can notice that the closer the prediction probability is to 1, the closer the loss is to 0, and in reverse, as the probability approaches 0, the loss function approaches $+\\infty$. \n"
      ],
      "metadata": {
        "id": "c6Id5Z2itrVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oneHot(Y):\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y"
      ],
      "metadata": {
        "id": "XpmlGc2i1Pvl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximizing cost function\n",
        "\n",
        "By maximizing the cost function, we improve the accuracy of our model. We do so by substrcting the derivative of the loss function with respect to eacch parameter from that parameter over many rounds of graident descent. \n",
        "\n",
        "$$\n",
        "W^{[1]} := W^{[1]} - \\alpha \\frac{{\\partial}J}{{\\partial}W^{[1]}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{[1]} := b^{[1]} - \\alpha \\frac{{\\partial}J}{{\\partial}b^{[1]}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{[2]} := W^{[2]} - \\alpha \\frac{{\\partial}J}{{\\partial}W^{[2]}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{[2]} := b^{[2]} - \\alpha \\frac{{\\partial}J}{{\\partial}b^{[2]}}\n",
        "$$"
      ],
      "metadata": {
        "id": "DOzIYNIR1MWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def updateParams(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1    \n",
        "    W2 = W2 - alpha * dW2  \n",
        "    b2 = b2 - alpha * db2    \n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "HsQrjl3n1180"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our goal with backward propagation is to find \n",
        "${\\frac{\\partial J}{\\partial W^{[1]}}}$, ${\\frac{\\partial J}{\\partial b^{[1]}}}$, ${\\frac{\\partial J}{\\partial W^{[2]}}}$, and ${\\frac{\\partial J}{\\partial b^{[2]}}}$, we have to first take a step backward in our NN layer and find ${\\frac{\\partial J}{\\partial A^{[2]}}}$. For the sake of simplicity, we will write these derivatives as $dW^{[1]}$, $db^{[1]}$, $dW^{[2]}$, $db^{[2]}$, and finally $dA^{[2]}$. This derivative is simply equal to $dA^{[2]} = Y - A^{[2]}$."
      ],
      "metadata": {
        "id": "1uAZjFrw2wdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From $dA^{[2]}$ we can calculate $dW^{[2]}$ and $db^{[2]}$ as follows:\n",
        "\n",
        "$$\n",
        "dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}\n",
        "$$\n",
        "\n",
        "$$\n",
        "db^{[2]} = \\frac{1}{m}{\\sum}dZ^{[2]} \n",
        "$$\n",
        "\n",
        "From here our goal is to calculate $dW^{[1]}$, and $db^{[1]}$ but first we need $dZ^{[1]}$. We find that using this formula:\n",
        "\n",
        "$$\n",
        "dZ^{[1]} = W^{[2]T}dZ^{[2]}. \\times g^{[1]'}(Z^{[1]})\n",
        "$$\n",
        "\n",
        "_In our formula_ $g^{[1]'}(Z^{[1]})$ _stands for our_ `DerivativeReLU(x)` _activation function._\n",
        "\n",
        "From here we can find $dW^{[1]}$, and $db^{[1]}$ by plugging $X$ in the formula instead of $A^{[1]}$. It will look like this:\n",
        "\n",
        "$$\n",
        "dW^{1]} = \\frac{1}{m}dZ^{[1]}X^{[1]T}\n",
        "$$\n",
        "\n",
        "$$\n",
        "db^{[1]} = \\frac{1}{m}{\\sum} dZ^{[1]} \n",
        "$$\n",
        "\n",
        "After completing this, our `backwardProp()` function doesn't do anything else but we still need to update our values using the `updateParams()` function. If we take a look at the `gradientDescent()` function, we can see that after each iteration we update the params.  "
      ],
      "metadata": {
        "id": "p7zbhvTO6h5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backwardProp(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    m = Y.size\n",
        "    one_hot_Y = oneHot(Y)\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLUDeriv(Z1)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "k1KCWSr52BsB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program logic\n",
        "\n",
        "Below we are going to talk about each function that isn't math based but is used for our NN. _As these functions don't do any of the calculations, and only handle the business logic there isn't too much to say._ "
      ],
      "metadata": {
        "id": "Q1Hd85lVGkbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `getPredictions()`\n",
        "\n",
        "This function takes the return value of our forward propogation `softmax()` function, which we represented as $A^{[2]}$. It returns the index of our predicted digit. "
      ],
      "metadata": {
        "id": "CeCZYAwTHASU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPredictions(A2):\n",
        "    return np.argmax(A2, 0)"
      ],
      "metadata": {
        "id": "tHONWqdQIJwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `getAccuracy()`\n",
        "\n",
        "This function takes our prediction and our raw data as parameters. It prints the prediction and the layer and returns the accuracy calculated. "
      ],
      "metadata": {
        "id": "eFPkWapwIMU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAccuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size"
      ],
      "metadata": {
        "id": "bU-qyVSkIMr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `gradientDescent()`\n",
        "\n",
        "This is our main function that wraps everything up together. It takes our training and testing data, our learning rate, and the number of iterations to complete. After every iteration it calls the `forwardProp()` function and proceeds to call the `backwardProp()` function. The inner workings of these functions was described above. After each iteration, the for loop updates our params considering our learning rate. \n",
        "\n",
        "After every 10th iteration, the program prints it's prediction and the accuracy of the prediction. "
      ],
      "metadata": {
        "id": "DV_DqBmjIObB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = initParams()\n",
        "    # for every iteration desired by the function argument\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forwardProp(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backwardProp(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = updateParams(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        # every 10th iteration, print accuracy\n",
        "        if i % 10 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = getPredictions(A2)\n",
        "            print(getAccuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "0lhaJ2Z_IOxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `gradientDescent()` function can be used as follows. It takes our X and Y NN data, our learning rate, and our iteration count."
      ],
      "metadata": {
        "id": "F8l5S-FRLLuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1, W2, b2 = gradientDescent(X_train, Y_train, 0.10, 500)"
      ],
      "metadata": {
        "id": "Cj_zqXkhIQsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to save $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, and $b^{[2]}$ so we can make our predictions accordingly. "
      ],
      "metadata": {
        "id": "32pjD7eIIR0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makePredictions(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forwardProp(W1, b1, W2, b2, X)\n",
        "    predictions = getPredictions(A2)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "kKDHd05MIStG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we only care about the value of $A^{[2]}$ in this case, we discard other return values from the tuple. Then we proceed to input $A^{[2]}$ in our `getPredictions()` function, and return the resulted prediction. "
      ],
      "metadata": {
        "id": "7aMPX1JQIUxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testPrediction(index, W1, b1, W2, b2):\n",
        "    current_image = X_train[:, index, None]\n",
        "    prediction = makePredictions(X_train[:, index, None], W1, b1, W2, b2)\n",
        "    label = Y_train[index]\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print(\"Label: \", label)\n",
        "    \n",
        "    current_image = current_image.reshape((28, 28)) * 255\n",
        "    plt.gray()\n",
        "    plt.imshow(current_image, interpolation='nearest')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "a_qoucxZiKN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the predictions made by our NN by supplying the index that we are interested in (in the range of 0, 9), and our $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, and $b^{[2]}$ values that were calculated by the `gradientDescent()` function. "
      ],
      "metadata": {
        "id": "WBHkLcKsMWLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testPrediction(3, W1, b1, W2, b2)"
      ],
      "metadata": {
        "id": "KrOejgbviLUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "I used [this](https://www.youtube.com/watch?v=9RN2Wr8xvro) video to help we have a better understanding of how this digit recognition NN would be constructed. "
      ],
      "metadata": {
        "id": "Zr3pUqifM_PY"
      }
    }
  ]
}